# pip install huggingface_hub pandas python-dateutil requests

import os, re, requests
import pandas as pd
from datetime import datetime, timezone
from dateutil.parser import isoparse
from huggingface_hub import HfApi
from time import sleep

# ------------------ Auth ------------------
HF_TOKEN = os.getenv("HF_TOKEN") or "token key"
api = HfApi(token=HF_TOKEN)

# ------------------ Limits & toggles ------------------
MAX_MODELS_SCAN = 2000
MAX_SPACES_SCAN  = 500
ENABLE_CONTRIBUTORS = True   # get individuals (slower)
MAX_COMMITS_PER_REPO = 25

# ------------------ Keywords ------------------
KEYWORDS = [
    "Cross-Encoder","Bi-encoder","reranker","sentence transformers",
    "language models in search","dense vector",
    "Quantization","LoRA","QLoRA","Low-Rank Adapters",
    "Semantic Embedding","sentence-similarity","feature-extraction",
    "vector search","retrieval","search ranking",
    "fine-tuning","PEFT","Prefix Tuning","P-Tuning","adapter"
]
KW_REGEX = re.compile("(" + "|".join(re.escape(k) for k in KEYWORDS) + ")", re.IGNORECASE)
def kw_match(s: str) -> bool: return bool(KW_REGEX.search(s or ""))

# ------------------ Helpers ------------------
def days_since(dt_str):
    if not dt_str: return None
    try:
        dt = isoparse(dt_str)
        return (datetime.now(timezone.utc) - dt).days
    except: return None

def recency_score(days):
    if days is None: return 0.0
    return max(0.0, 1.0 - min(days, 180) / 180.0)

def profile_url_for(ns: str) -> str:
    return f"https://huggingface.co/{ns}" if ns else ""

def repo_url_for(repo_id: str, rtype: str) -> str:
    if not repo_id: return ""
    return f"https://huggingface.co/spaces/{repo_id}" if rtype == "space" else f"https://huggingface.co/{repo_id}"

def fetch_display_name(ns: str) -> str:
    """Public display name for a namespace (user or org)."""
    if not ns: return ""
    try:
        headers = {"Authorization": f"Bearer {HF_TOKEN}"} if HF_TOKEN else {}
        r = requests.get(f"https://huggingface.co/api/users/{ns}", headers=headers, timeout=8)
        if r.ok:
            j = r.json()
            return j.get("name") or j.get("display_name") or ""
    except Exception:
        pass
    return ""

def list_contribs(repo_id, max_commits=MAX_COMMITS_PER_REPO):
    try:
        commits = list(api.list_repo_commits(repo_id))[:max_commits]
    except Exception:
        return "", ""
    people = []
    for c in commits:
        for entry in (getattr(c, "author", None), getattr(c, "committer", None)):
            if not entry: continue
            uname = getattr(entry, "login", None) or getattr(entry, "username", None) or (entry.get("username") if isinstance(entry, dict) else None)
            name  = getattr(entry, "name", None)  or (entry.get("name") if isinstance(entry, dict) else None)
            if uname or name:
                people.append((uname or name, f"https://huggingface.co/{uname}" if uname else ""))
    # dedupe
    seen, rows = set(), []
    for u, link in people:
        if u and u not in seen:
            seen.add(u); rows.append((u, link))
    users = ", ".join([u for u,_ in rows])
    links = ", ".join([l for _,l in rows if l])
    return users, links

# ------------------ Scan Models ------------------
print("üîé Scanning models...")
model_rows = []
for i, m in enumerate(api.list_models()):
    if i >= MAX_MODELS_SCAN: break
    tags = m.tags or []
    searchable = " ".join([m.modelId or "", getattr(m,"pipeline_tag","") or "", " ".join(tags)])
    if not kw_match(searchable): continue

    model_rows.append({
        "group_or_org": m.author or "",
        "repo_id": m.modelId,
        "type": "model",
        "task": getattr(m,"pipeline_tag","") or "",
        "tags": ",".join(tags),
        "likes": getattr(m,"likes",0) or 0,
        "downloads": getattr(m,"downloads",0) or 0,
        "last_modified_days": days_since(getattr(m,"lastModified",None)),
    })
print(f"   scanned {i+1}, kept {len(model_rows)}")

# ------------------ Scan Spaces ------------------
print("üîé Scanning spaces...")
space_rows = []
for j, s in enumerate(api.list_spaces()):
    if j >= MAX_SPACES_SCAN: break
    tags = getattr(s,"tags",[]) or []
    searchable = " ".join([s.id or "", getattr(s,"sdk","") or "", " ".join(tags)])
    if not kw_match(searchable): continue

    space_rows.append({
        "group_or_org": s.author or "",
        "repo_id": s.id,
        "type": "space",
        "task": "",
        "tags": ",".join(tags),
        "likes": getattr(s,"likes",0) or 0,
        "downloads": 0,
        "last_modified_days": days_since(getattr(s,"lastModified",None)),
    })
print(f"   scanned {j+1}, kept {len(space_rows)}")

# ------------------ Combine ------------------
df = pd.DataFrame(model_rows + space_rows)
if df.empty:
    print("‚ö†Ô∏è No repos matched.")
    pd.DataFrame().to_csv("hf_candidates_details.csv", index=False)
    print("‚úÖ Saved empty hf_candidates_details.csv (no matches).")
    raise SystemExit

# Fill org/user + URLs
df["group_or_org"] = df.apply(
    lambda row: row["repo_id"].split("/")[0] if (not row["group_or_org"] and "/" in str(row["repo_id"])) else row["group_or_org"],
    axis=1
)
df["group_profile_url"] = df["group_or_org"].apply(profile_url_for)
df["repo_url"] = df.apply(lambda r: repo_url_for(r["repo_id"], r["type"]), axis=1)
df["group_display_name"] = df["group_or_org"].apply(fetch_display_name)

# Contributors (if enabled)
if ENABLE_CONTRIBUTORS:
    contrib_users, contrib_links = [], []
    for rid in df["repo_id"]:
        u, l = list_contribs(rid, MAX_COMMITS_PER_REPO)
        contrib_users.append(u); contrib_links.append(l); sleep(0.15)
    df["contributors_usernames"] = contrib_users
    df["contributors_links"] = contrib_links

# ------------------ Save to CSV ------------------
output_file = "hf_candidates_details.csv"
df.to_csv(output_file, index=False)
print(f"‚úÖ Done! Saved results to {output_file}")
